"""
================================================================================
THAI INTENT DATASET GENERATOR - SEQUENTIAL AUGMENTATION PIPELINE
================================================================================
Pipeline Flow:
    Original Sentence
         ‚Üì
    Back-Translation (th‚Üíen‚Üíth)
         ‚Üì
    Thai2Fit fastText (synonym replacement)
         ‚Üì
    Thai2Fit ULMFit (contextual generation)
         ‚Üì
    Filter Duplicates + Check Intent Consistency
================================================================================
"""

import pandas as pd
import random
import string
import re
import os
from typing import List, Dict, Tuple, Set
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')
import nltk
# ============================================================================
# 1. BACKTRANSLATION MODULE
# ============================================================================

class BacktranslateAugmenter:
    """Step 1: Back-translation (th‚Üíen‚Üíth)"""
    
    def __init__(self, use_gpu: bool = False):
        try:
            from transformers import MarianMTModel, MarianTokenizer
            import torch
            
            self.device = "cuda" if use_gpu and torch.cuda.is_available() else "cpu"
            print(f"üîÑ [1/4] Loading Backtranslate models (device: {self.device})...")
            
            # Thai ‚Üí English
            self.th2en_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-th-en")
            self.th2en_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-th-en").to(self.device)
            
            # English ‚Üí Thai
            from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
            print("   Using mBART50 for en‚Üíth translation...")
            self.en2th_tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
            self.en2th_model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt").to(self.device)
            self.use_mbart = True
            
            self.available = True
            print("   ‚úÖ Backtranslate ready")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Backtranslate unavailable: {e}")
            self.available = False
    
    def augment(self, text: str) -> str:
        """Back-translate: Thai ‚Üí English ‚Üí Thai"""
        if not self.available or len(text.strip()) < 3:
            return text
        
        try:
            # Thai ‚Üí English
            inputs = self.th2en_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            translated = self.th2en_model.generate(**inputs, max_length=512)
            en_text = self.th2en_tokenizer.decode(translated[0], skip_special_tokens=True)
            
            # English ‚Üí Thai
            inputs = self.en2th_tokenizer(en_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            translated = self.en2th_model.generate(**inputs, max_length=512)
            th_text = self.en2th_tokenizer.decode(translated[0], skip_special_tokens=True)
            
            return th_text.strip()
        except Exception as e:
            return text


# ============================================================================
# 2. THAI2FIT FASTTEXT MODULE (Synonym Replacement)
# ============================================================================

class Thai2FitFastText:
    """Step 2: Thai2Fit fastText synonym replacement"""
    
    def __init__(self):
        print("üìö [2/4] Loading Thai2Fit fastText (word embeddings)...")
        
        try:
            from pythainlp.word_vector import get_model
            from pythainlp import word_tokenize
            
            # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Thai2Fit
            print("   üì• Downloading Thai2Fit model...")
            self.word2vec = get_model("thai2fit_wv")  # ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà
            self.tokenizer = word_tokenize
            self.available = True
            print("   ‚úÖ Thai2Fit fastText ready")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Thai2Fit fastText unavailable: {e}")
            print("   üí° Install: pip install pythainlp gensim")
            self.available = False
            
            # Fallback dictionary
            self.fallback_dict = {
                "‡∏Ç‡∏≠": ["‡∏≠‡∏¢‡∏≤‡∏Å", "‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"], "‡∏≠‡∏¢‡∏≤‡∏Å": ["‡∏Ç‡∏≠", "‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"],
                "‡∏ã‡∏∑‡πâ‡∏≠": ["‡∏™‡∏±‡πà‡∏á", "‡∏à‡∏≠‡∏á"], "‡∏™‡∏±‡πà‡∏á": ["‡∏ã‡∏∑‡πâ‡∏≠", "‡∏à‡∏≠‡∏á"],
                "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤": ["‡∏Ç‡∏≠‡∏á", "‡πÑ‡∏≠‡πÄ‡∏ó‡∏°"], "‡∏£‡∏≤‡∏Ñ‡∏≤": ["‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà", "‡∏Å‡∏µ‡πà‡∏ö‡∏≤‡∏ó"],
                "‡∏î‡∏µ": ["‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°", "‡πÄ‡∏à‡πã‡∏á"], "‡∏™‡∏ß‡∏¢": ["‡∏á‡∏≤‡∏°", "‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å"],
                "‡∏°‡∏≤‡∏Å": ["‡πÄ‡∏•‡∏¢", "‡∏à‡∏±‡∏á"], "‡∏´‡∏ô‡πà‡∏≠‡∏¢": ["‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡∏ö‡πâ‡∏≤‡∏á"],
            }
    
    def get_similar_words(self, word: str, topn: int = 5) -> List[str]:
        """‡∏´‡∏≤‡∏Ñ‡∏≥‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å word embeddings"""
        if not self.available:
            return self.fallback_dict.get(word, [])
        
        try:
            similar = self.word2vec.most_similar(word, topn=topn)
            return [w[0] for w in similar if len(w[0]) >= 2]
        except:
            return self.fallback_dict.get(word, [])
    
    def augment(self, text: str, num_replacements: int = 2) -> str:
        """‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ synonym (‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢)"""
        if len(text.strip()) < 3:
            return text
        
        try:
            # Tokenize
            if self.available:
                tokens = self.tokenizer(text, engine='newmm')
            else:
                tokens = text.split()
            
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà
            candidates = [i for i, t in enumerate(tokens) if len(t) >= 2 and t.isalpha()]
            if not candidates:
                return text
            
            # Random select tokens to replace
            num_to_replace = min(num_replacements, len(candidates))
            indices_to_replace = random.sample(candidates, num_to_replace)
            
            # Replace
            for idx in indices_to_replace:
                token = tokens[idx]
                similar_words = self.get_similar_words(token, topn=5)
                
                if similar_words:
                    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
                    suitable = [w for w in similar_words if abs(len(w) - len(token)) <= 3]
                    if suitable:
                        tokens[idx] = random.choice(suitable[:3])
            
            return ''.join(tokens) if self.available else ' '.join(tokens)
        except:
            return text


# ============================================================================
# 3. THAI2FIT ULMFIT MODULE (Contextual Generation)
# ============================================================================

class Thai2FitULMFit:
    """Step 3: Thai2Fit ULMFit contextual text generation"""
    
    def __init__(self):
        print("üß† [3/4] Loading Thai2Fit ULMFit (language model)...")
        
        try:
            from pythainlp.augment.lm import Thai2fitAug
            
            self.augmenter = Thai2fitAug()
            self.available = True
            print("   ‚úÖ Thai2Fit ULMFit ready")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Thai2Fit ULMFit unavailable: {e}")
            print("   üí° This uses pythainlp's language model")
            self.available = False
    
    def augment(self, text: str) -> str:
        """Generate contextual variations"""
        if not self.available or len(text.strip()) < 5:
            return text
        
        try:
            # ULMFit ‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
            augmented = self.augmenter.augment(text, n_sent=1)
            return augmented[0] if augmented else text
        except:
            return text


# ============================================================================
# 4. QUALITY FILTER MODULE
# ============================================================================

class QualityFilter:
    """Step 4: Filter duplicates + Check intent consistency"""
    
    def __init__(self, intent_keywords: Dict[str, List[str]]):
        print("üîç [4/4] Initializing Quality Filter...")
        self.intent_keywords = intent_keywords
        self.seen_texts = set()
        print("   ‚úÖ Quality Filter ready")
    
    def normalize_text(self, text: str) -> str:
        """Normalize ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö"""
        # ‡∏•‡∏ö whitespace ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô
        text = re.sub(r'\s+', ' ', text.strip())
        # ‡∏•‡∏ö punctuation
        text = re.sub(r'[^\w\s]', '', text)
        # lowercase
        return text.lower()
    
    def is_duplicate(self, text: str, threshold: float = 0.9) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö duplicate (exact match ‡πÅ‡∏•‡∏∞ normalized)"""
        normalized = self.normalize_text(text)
        
        # Exact match
        if normalized in self.seen_texts:
            return True
        
        # Similarity check (optional - ‡πÉ‡∏ä‡πâ Levenshtein distance)
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö demo ‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ exact match ‡∏Å‡πà‡∏≠‡∏ô
        
        self.seen_texts.add(normalized)
        return False
    
    def check_intent_consistency(self, text: str, intent: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ augmented text ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á intent ‡πÄ‡∏î‡∏¥‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        if intent not in self.intent_keywords:
            return True  # unknown intent ‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô
        
        keywords = self.intent_keywords[intent]
        text_lower = text.lower()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á intent ‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        matches = sum(1 for kw in keywords if kw in text_lower)
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 keyword ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤ text ‡∏™‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å (< 5 chars) ‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô
        if matches > 0 or len(text.strip()) < 5:
            return True
        
        return False
    
    def filter(self, text: str, intent: str) -> Tuple[bool, str]:
        """
        Filter augmented text
        Returns: (is_valid, reason)
        """
        # Check 1: Too short or too long
        if len(text.strip()) < 2:
            return False, "too_short"
        if len(text.strip()) > 500:
            return False, "too_long"
        
        # Check 2: Duplicate
        if self.is_duplicate(text):
            return False, "duplicate"
        
        # Check 3: Intent consistency
        if not self.check_intent_consistency(text, intent):
            return False, "intent_mismatch"
        
        # Check 4: Too many repeated characters (spam)
        if re.search(r'(.)\1{5,}', text):
            return False, "spam_chars"
        
        return True, "valid"


# ============================================================================
# 5. SEQUENTIAL AUGMENTATION PIPELINE
# ============================================================================

class SequentialAugmentationPipeline:
    """Main pipeline that chains all augmentation steps"""
    
    def __init__(
        self,
        use_backtranslate: bool = True,
        use_fasttext: bool = True,
        use_ulmfit: bool = True,
        use_gpu: bool = False
    ):
        print("\n" + "="*80)
        print("üöÄ INITIALIZING SEQUENTIAL AUGMENTATION PIPELINE")
        print("="*80 + "\n")
        
        # Initialize augmenters
        self.backtranslate = BacktranslateAugmenter(use_gpu) if use_backtranslate else None
        self.fasttext = Thai2FitFastText() if use_fasttext else None
        self.ulmfit = Thai2FitULMFit() if use_ulmfit else None
        
        # Intent keywords for filtering
        self.intent_keywords = {
            "ask_info": ["‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", "‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°", "‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ", "spec", "‡πÇ‡∏õ‡∏£", "‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î"],
            "order_product": ["‡∏ã‡∏∑‡πâ‡∏≠", "‡∏™‡∏±‡πà‡∏á", "‡∏à‡∏≠‡∏á", "order", "cf", "‡πÄ‡∏≠‡∏≤", "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ"],
            "refund_request": ["‡∏Ñ‡∏∑‡∏ô", "‡πÄ‡∏á‡∏¥‡∏ô", "‡∏Ç‡∏≠‡∏á", "‡∏õ‡∏±‡∏ç‡∏´‡∏≤", "‡πÄ‡∏™‡∏µ‡∏¢", "‡∏ú‡∏¥‡∏î", "refund", "‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô"],
            "help_request": ["‡∏ä‡πà‡∏ß‡∏¢", "help", "‡∏õ‡∏±‡∏ç‡∏´‡∏≤", "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠", "‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô", "error", "‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ"],
            "greeting": ["‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏´‡∏ß‡∏±‡∏î", "hello", "hi", "‡∏î‡∏µ"],
            "thank_you": ["‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏Ç‡∏≠‡∏ö‡πÉ‡∏à", "thank"],
            "feedback": ["‡∏î‡∏µ", "‡∏ä‡∏≠‡∏ö", "‡πÅ‡∏¢‡πà", "‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à", "‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤"],
        }
        
        self.filter = QualityFilter(self.intent_keywords)
        
        print("\n" + "="*80)
        print("‚úÖ PIPELINE READY")
        print("="*80 + "\n")
    
    def augment_single(
        self,
        text: str,
        intent: str,
        apply_backtranslate: bool = True,
        apply_fasttext: bool = True,
        apply_ulmfit: bool = True
    ) -> List[Dict[str, str]]:
        """
        Augment single text through the pipeline
        
        Returns list of augmented variants with metadata
        """
        results = []
        current_text = text
        
        # Original
        results.append({
            "text": current_text,
            "stage": "original",
            "intent": intent
        })
        
        # Stage 1: Backtranslation
        if apply_backtranslate and self.backtranslate and self.backtranslate.available:
            bt_text = self.backtranslate.augment(current_text)
            if bt_text != current_text:
                is_valid, reason = self.filter.filter(bt_text, intent)
                if is_valid:
                    results.append({
                        "text": bt_text,
                        "stage": "backtranslate",
                        "intent": intent
                    })
                    current_text = bt_text  # Use for next stage
        
        # Stage 2: fastText (synonym replacement)
        if apply_fasttext and self.fasttext:
            ft_text = self.fasttext.augment(current_text, num_replacements=2)
            if ft_text != current_text:
                is_valid, reason = self.filter.filter(ft_text, intent)
                if is_valid:
                    results.append({
                        "text": ft_text,
                        "stage": "fasttext",
                        "intent": intent
                    })
                    current_text = ft_text  # Use for next stage
        
        # Stage 3: ULMFit (contextual generation)
        if apply_ulmfit and self.ulmfit and self.ulmfit.available:
            ulm_text = self.ulmfit.augment(current_text)
            if ulm_text != current_text:
                is_valid, reason = self.filter.filter(ulm_text, intent)
                if is_valid:
                    results.append({
                        "text": ulm_text,
                        "stage": "ulmfit",
                        "intent": intent
                    })
        
        return results
    
    def augment_batch(
        self,
        texts: List[str],
        intents: List[str],
        augment_prob: float = 0.5,
        max_variants_per_text: int = 3
    ) -> List[Dict[str, str]]:
        """
        Augment batch of texts
        
        Args:
            texts: List of original texts
            intents: List of corresponding intents
            augment_prob: Probability of augmenting each text
            max_variants_per_text: Maximum augmented variants per original
        """
        all_results = []
        
        for text, intent in zip(texts, intents):
            # Always include original
            all_results.append({
                "text": text,
                "stage": "original",
                "intent": intent
            })
            
            # Augment with probability
            if random.random() < augment_prob:
                # Randomly decide which stages to apply
                apply_bt = random.random() < 0.3  # 30% chance
                apply_ft = random.random() < 0.7  # 70% chance
                apply_ulm = random.random() < 0.4  # 40% chance
                
                augmented = self.augment_single(
                    text, intent,
                    apply_backtranslate=apply_bt,
                    apply_fasttext=apply_ft,
                    apply_ulmfit=apply_ulm
                )
                
                # Add augmented variants (skip original)
                variants = augmented[1:]  # Skip original
                if variants:
                    # Limit variants
                    selected = random.sample(variants, min(len(variants), max_variants_per_text))
                    all_results.extend(selected)
        
        return all_results


# ============================================================================
# 6. INTENT TEMPLATES
# ============================================================================

intents_templates = {
    "ask_info": [
        "‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏Ç‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤", "‡πÄ‡∏≠‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏°‡∏µ‡πÇ‡∏õ‡∏£‡πÑ‡∏´‡∏°",
        "‡∏Ç‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏£‡∏≤‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ô‡∏µ‡πâ"
    ],
    "order_product": [
        "‡∏ã‡∏∑‡πâ‡∏≠", "‡∏™‡∏±‡πà‡∏á", "‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏•‡∏¢", "CF ‡∏Ñ‡πà‡∏∞", "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ô‡∏µ‡πâ",
        "‡∏Ç‡∏≠‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ä‡∏¥‡πâ‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö"
    ],
    "refund_request": [
        "‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏á‡∏¥‡∏ô", "‡∏Ç‡∏≠‡∏á‡∏û‡∏±‡∏á", "‡∏Ç‡∏≠‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏á‡∏¥‡∏ô", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏™‡∏µ‡∏¢", "‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤",
        "‡∏≠‡∏¢‡∏≤‡∏Å‡∏Ç‡∏≠‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤"
    ],
    "help_request": [
        "‡∏ä‡πà‡∏ß‡∏¢", "help", "‡∏ä‡πà‡∏ß‡∏¢‡∏î‡πâ‡∏ß‡∏¢", "‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤", "‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ",
        "‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö"
    ],
    "greeting": [
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏´‡∏ß‡∏±‡∏î‡∏î‡∏µ", "Hello"
    ],
    "thank_you": [
        "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏±‡∏ö", "Thanks", "‡∏Ç‡∏≠‡∏ö‡πÉ‡∏à"
    ],
    "feedback": [
        "‡∏î‡∏µ‡∏°‡∏≤‡∏Å", "‡∏ä‡∏≠‡∏ö", "‡∏Ç‡∏≠‡∏á‡∏î‡∏µ", "‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏µ", "‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à"
    ],
    "unknown": [
        "‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏¢‡∏±‡∏á", "‡∏ù‡∏ô‡∏ï‡∏Å‡∏´‡∏ô‡∏±‡∏Å", "555555", "asdfgh"
    ]
}


# ============================================================================
# 7. MAIN DATASET GENERATOR
# ============================================================================

def generate_sequential_augmented_dataset(
    target_total: int = 40000,
    augment_prob: float = 0.5,
    max_variants: int = 3,
    use_backtranslate: bool = True,
    use_fasttext: bool = True,
    use_ulmfit: bool = True,
    use_gpu: bool = False,
    save_path: str = "Data/intents_dataset_sequential.csv"
):
    """
    Generate dataset using sequential augmentation pipeline
    
    Args:
        target_total: Target number of samples
        augment_prob: Probability of augmenting each sample
        max_variants: Max augmented variants per original
        use_backtranslate: Enable backtranslation
        use_fasttext: Enable Thai2Fit fastText
        use_ulmfit: Enable Thai2Fit ULMFit
        use_gpu: Use GPU for models
        save_path: Output file path
    """
    
    print("\n" + "="*80)
    print("üìä GENERATING SEQUENTIAL AUGMENTED DATASET")
    print("="*80)
    print(f"\nTarget: {target_total:,} samples")
    print(f"Augmentation probability: {augment_prob:.1%}")
    print(f"Max variants per text: {max_variants}")
    print(f"Backtranslate: {'‚úì' if use_backtranslate else '‚úó'}")
    print(f"FastText: {'‚úì' if use_fasttext else '‚úó'}")
    print(f"ULMFit: {'‚úì' if use_ulmfit else '‚úó'}")
    print(f"GPU: {'‚úì' if use_gpu else '‚úó'}\n")
    
    # Initialize pipeline
    pipeline = SequentialAugmentationPipeline(
        use_backtranslate=use_backtranslate,
        use_fasttext=use_fasttext,
        use_ulmfit=use_ulmfit,
        use_gpu=use_gpu
    )
    
    # Generate base samples
    base_samples = []
    base_per_intent = target_total // (len(intents_templates) * (1 + max_variants))
    
    print(f"üîß Generating base samples (~{base_per_intent} per intent)...\n")
    
    for intent, templates in intents_templates.items():
        for _ in range(base_per_intent):
            text = random.choice(templates)
            base_samples.append((text, intent))
    
    # Augment
    print(f"üîÑ Running augmentation pipeline...\n")
    
    augmented_results = pipeline.augment_batch(
        texts=[t[0] for t in base_samples],
        intents=[t[1] for t in base_samples],
        augment_prob=augment_prob,
        max_variants_per_text=max_variants
    )
    
    # Create DataFrame
    df = pd.DataFrame(augmented_results)
    
    # Add metadata
    df['word_count'] = df['text'].apply(lambda x: len(x.split()))
    df['char_count'] = df['text'].apply(lambda x: len(x))
    df['platform'] = random.choices(
        ["LINE", "Facebook", "Web", "Instagram"],
        k=len(df)
    )
    
    # Shuffle
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    # Limit to target
    if len(df) > target_total:
        df = df.iloc[:target_total]
    
    # Save
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    df.to_csv(save_path, index=False, encoding="utf-8-sig")
    
    # Statistics
    print("\n" + "="*80)
    print("‚úÖ DATASET GENERATED")
    print("="*80)
    print(f"\nüìÅ Saved: {save_path}")
    print(f"üìä Total samples: {len(df):,}")
    
    print(f"\nüìã Intent Distribution:")
    print(df['intent'].value_counts().to_string())
    
    print(f"\nüîÑ Augmentation Stage Distribution:")
    print(df['stage'].value_counts().to_string())
    
    print(f"\nüìè Length Statistics:")
    print(f"   Words: min={df['word_count'].min()}, max={df['word_count'].max()}, mean={df['word_count'].mean():.1f}")
    print(f"   Chars: min={df['char_count'].min()}, max={df['char_count'].max()}, mean={df['char_count'].mean():.1f}")
    
    # Show examples by stage
    print(f"\nüîç EXAMPLES BY STAGE:\n")
    for stage in ['original', 'backtranslate', 'fasttext', 'ulmfit']:
        stage_df = df[df['stage'] == stage]
        if len(stage_df) > 0:
            print(f"--- {stage.upper()} ---")
            for _, row in stage_df.head(3).iterrows():
                print(f"[{row['intent']}] {row['text']}")
            print()
    
    print("="*80 + "\n")
    
    return df


# ============================================================================
# 8. EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    
    # Generate dataset with full pipeline
    df = generate_sequential_augmented_dataset(
        target_total=30000,
        augment_prob=0.3,        # 50% ‡∏Ç‡∏≠‡∏á samples ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å augment
        max_variants=3,          # ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 3 variants ‡∏ï‡πà‡∏≠ original
        use_backtranslate=True,  # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ backtranslate
        use_fasttext=True,       # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ fastText
        use_ulmfit=True,         # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ ULMFit
        use_gpu=False,           # True ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ GPU
        save_path="../Train_model_IntentData/intents_dataset_sequential.csv"
    )
    
    print("üéâ Dataset generation complete!")
    print(f"üìà Final dataset size: {len(df):,} samples")
    print(f"üìä Augmented samples: {len(df[df['stage'] != 'original']):,}")