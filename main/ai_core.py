"""
AI Core Service - ‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
‡∏£‡∏±‡∏ô: uvicorn ai_core:app --host 0.0.0.0 --port 8001 --reload
"""
import os
import pickle
import requests
import numpy as np
from datetime import datetime
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from dotenv import load_dotenv
import re

# ‚ú® ‡πÉ‡∏ä‡πâ PyThaiNLP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
try:
    from pythainlp.tokenize import word_tokenize
    USE_PYTHAINLP = True
    print("‚úÖ PyThaiNLP loaded successfully")
except ImportError:
    USE_PYTHAINLP = False
    print("‚ö†Ô∏è PyThaiNLP not found - using simple tokenizer")

# ‡πÇ‡∏´‡∏•‡∏î .env
load_dotenv(override=True)

app = FastAPI(title="AI Core Service", version="1.0.0")

# ===== GLOBAL STATE =====
MODEL_PATH = os.getenv('MODEL_PATH')
intent_model = None
model_components = None


# ===== REQUEST/RESPONSE MODELS =====
class ChatRequest(BaseModel):
    message: str
    user_id: str = "default"
    platform: str = "web"  # line, web, app
    context: dict = {}


class ChatResponse(BaseModel):
    response: str
    intent: str
    confidence: float = 1.0
    data: dict = {}


# ===== MOCK DATABASE =====
MOCK_PRODUCTS = [
    {
        "product_id": 1,
        "product_name": "‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏¢‡∏∑‡∏î‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß",
        "description": "‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏¢‡∏∑‡∏î‡∏Ñ‡∏≠‡∏Å‡∏•‡∏°‡∏ú‡πâ‡∏≤‡∏Ñ‡∏≠‡∏ï‡∏ï‡∏≠‡∏ô 100% ‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß ‡πÉ‡∏™‡πà‡∏™‡∏ö‡∏≤‡∏¢",
        "price": 299,
        "stock": 50,
        "keywords": ["‡πÄ‡∏™‡∏∑‡πâ‡∏≠", "‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏¢‡∏∑‡∏î", "‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß", "‡∏ú‡πâ‡∏≤", "‡∏Ñ‡∏≠‡∏ï‡∏ï‡∏≠‡∏ô"],
    },
    {
        "product_id": 2,
        "product_name": "‡∏Å‡∏≤‡∏á‡πÄ‡∏Å‡∏á‡∏¢‡∏µ‡∏ô‡∏™‡πå",
        "description": "‡∏Å‡∏≤‡∏á‡πÄ‡∏Å‡∏á‡∏¢‡∏µ‡∏ô‡∏™‡πå‡∏Ç‡∏≤‡∏¢‡∏≤‡∏ß ‡∏ó‡∏£‡∏á‡∏™‡∏•‡∏¥‡∏° ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏°",
        "price": 890,
        "stock": 30,
        "keywords": ["‡∏Å‡∏≤‡∏á‡πÄ‡∏Å‡∏á", "‡∏¢‡∏µ‡∏ô‡∏™‡πå", "‡∏Ç‡∏≤‡∏¢‡∏≤‡∏ß", "‡∏™‡∏•‡∏¥‡∏°", "‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô"],
    },
    {
        "product_id": 3,
        "product_name": "‡∏£‡∏≠‡∏á‡πÄ‡∏ó‡πâ‡∏≤‡∏ú‡πâ‡∏≤‡πÉ‡∏ö",
        "description": "‡∏£‡∏≠‡∏á‡πÄ‡∏ó‡πâ‡∏≤‡∏ú‡πâ‡∏≤‡πÉ‡∏ö‡∏™‡∏µ‡∏î‡∏≥ ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏ö‡∏≤ ‡πÉ‡∏™‡πà‡∏™‡∏ö‡∏≤‡∏¢",
        "price": 1290,
        "stock": 20,
        "keywords": ["‡∏£‡∏≠‡∏á‡πÄ‡∏ó‡πâ‡∏≤", "‡∏ú‡πâ‡∏≤‡πÉ‡∏ö", "‡∏™‡∏µ‡∏î‡∏≥", "‡πÄ‡∏ö‡∏≤"],
    },
    {
        "product_id": 4,
        "product_name": "‡∏Å‡∏£‡∏∞‡πÄ‡∏õ‡πã‡∏≤‡∏™‡∏∞‡∏û‡∏≤‡∏¢‡∏Ç‡πâ‡∏≤‡∏á",
        "description": "‡∏Å‡∏£‡∏∞‡πÄ‡∏õ‡πã‡∏≤‡∏ú‡πâ‡∏≤‡πÅ‡∏Ñ‡∏ô‡∏ß‡∏≤‡∏™ ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏•‡∏≤‡∏á ‡∏™‡∏∞‡∏û‡∏≤‡∏¢‡∏Ç‡πâ‡∏≤‡∏á‡∏™‡∏ö‡∏≤‡∏¢",
        "price": 450,
        "stock": 15,
        "keywords": ["‡∏Å‡∏£‡∏∞‡πÄ‡∏õ‡πã‡∏≤", "‡∏™‡∏∞‡∏û‡∏≤‡∏¢", "‡πÅ‡∏Ñ‡∏ô‡∏ß‡∏≤‡∏™"],
    },
    {
        "product_id": 5,
        "product_name": "‡∏´‡∏°‡∏ß‡∏Å‡πÅ‡∏Å‡πä‡∏õ",
        "description": "‡∏´‡∏°‡∏ß‡∏Å‡πÅ‡∏Å‡πä‡∏õ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏î‡πâ ‡∏ú‡πâ‡∏≤‡∏Ñ‡∏≠‡∏ï‡∏ï‡∏≠‡∏ô",
        "price": 250,
        "stock": 40,
        "keywords": ["‡∏´‡∏°‡∏ß‡∏Å", "‡πÅ‡∏Å‡πä‡∏õ", "‡∏Ñ‡∏≠‡∏ï‡∏ï‡∏≠‡∏ô"],
    }
]

MOCK_ORDERS = {
    "U1234567890": [
        {
            "order_id": "ORD001",
            "product_name": "‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏¢‡∏∑‡∏î‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß",
            "status": "‡∏à‡∏±‡∏î‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß",
            "created_at": "2025-10-15 10:30:00",
        }
    ],
    "default": [
        {
            "order_id": "ORD999",
            "product_name": "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á",
            "status": "‡∏£‡∏≠‡∏ä‡∏≥‡∏£‡∏∞‡πÄ‡∏á‡∏¥‡∏ô",
            "created_at": "2025-10-17 09:00:00",
        }
    ]
}


# ===== STARTUP - LOAD MODEL ONCE =====
@app.on_event("startup")
async def load_model():
    """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡∏≠‡∏ô startup"""
    global intent_model, model_components
    
    try:
        with open(MODEL_PATH, 'rb') as f:
            loaded_data = pickle.load(f)
        
        if isinstance(loaded_data, dict):
            model_components = loaded_data
            intent_model = 'hybrid'
            print(f"‚úÖ Loaded hybrid model from: {MODEL_PATH}")
        else:
            intent_model = loaded_data
            print(f"‚úÖ Loaded intent model from: {MODEL_PATH}")
    except Exception as e:
        print(f"‚ö†Ô∏è Model not found ({e}) - using rule-based")
        intent_model = 'rule-based'


# ===== UTILITY FUNCTIONS =====
def thai_tokenize(text):
    """‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
    if USE_PYTHAINLP:
        tokens = word_tokenize(text, engine='newmm', keep_whitespace=False)
        tokens = [t for t in tokens if len(t.strip()) >= 2 and not t.isspace()]
        return tokens
    else:
        THAI_DICT = [
            '‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏Å‡∏≤‡∏á‡πÄ‡∏Å‡∏á', '‡πÄ‡∏™‡∏∑‡πâ‡∏≠', '‡∏£‡∏≠‡∏á‡πÄ‡∏ó‡πâ‡∏≤',
            '‡∏Å‡∏£‡∏∞‡πÄ‡∏õ‡πã‡∏≤', '‡∏´‡∏°‡∏ß‡∏Å', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏™‡∏ï‡πá‡∏≠‡∏Å', '‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠', '‡∏Ñ‡∏∑‡∏ô‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤',
            '‡∏¢‡∏µ‡∏ô‡∏™‡πå', '‡∏ú‡πâ‡∏≤‡πÉ‡∏ö', '‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß', '‡∏™‡∏µ‡∏î‡∏≥', '‡∏Ñ‡∏≠‡∏ï‡∏ï‡∏≠‡∏ô', '‡∏Ç‡∏≠', '‡∏î‡∏π', '‡∏°‡∏µ'
        ]
        THAI_DICT.sort(key=len, reverse=True)
        text = re.sub(r'[^\u0E00-\u0E7Fa-zA-Z0-9\s]', '', text.lower())
        found = []
        i = 0
        while i < len(text):
            matched = False
            for word in THAI_DICT:
                if text[i:i+len(word)] == word:
                    found.append(word)
                    i += len(word)
                    matched = True
                    break
            if not matched:
                i += 1
        return list(set(found))


def get_product_info(product_name=None, limit=5):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤"""
    if product_name:
        search_terms = thai_tokenize(product_name)
        if not search_terms:
            search_terms = [product_name.lower()]
        
        results = []
        for p in MOCK_PRODUCTS:
            searchable = (
                f"{p['product_name']} {p['description']} "
                f"{' '.join(p.get('keywords', []))}"
            ).lower()
            match_count = sum(1 for t in search_terms if t in searchable)
            if match_count > 0:
                results.append((p, match_count))
        
        results.sort(key=lambda x: x[1], reverse=True)
        return [p[0] for p in results[:limit]]
    return MOCK_PRODUCTS[:limit]


def get_order_info(user_id=None, order_id=None):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠"""
    if order_id:
        for orders in MOCK_ORDERS.values():
            for o in orders:
                if o['order_id'] == order_id:
                    return [o]
        return []
    elif user_id:
        return MOCK_ORDERS.get(user_id, MOCK_ORDERS.get("default", []))[:5]
    return []


def call_ollama_llm(prompt, context=""):
    """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏à‡∏≤‡∏Å Ollama"""
    try:
        model_name = os.getenv('OLLAMA_MODEL', 'llama3.2')
        system_prompt = f"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏£‡πâ‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ 2-3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ\n\n‡∏ö‡∏£‡∏¥‡∏ö‡∏ó:\n{context}\n\n‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {prompt}"
        
        payload = {
            "model": model_name,
            "prompt": system_prompt,
            "stream": False
        }
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            return response.json().get("response", "").strip()
        return None
    except Exception as e:
        print(f"‚ùå Ollama Error: {e}")
        return None


def predict_intent(text):
    """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Intent"""
    global intent_model, model_components
    
    if intent_model == 'hybrid' and model_components:
        try:
            semantic_model = model_components['semantic']
            tfidf_vectorizer = model_components['tfidf']
            clf = model_components['clf']
            
            semantic_features = semantic_model.encode([text])
            tfidf_features = tfidf_vectorizer.transform([text]).toarray()
            
            scaler_sem = model_components.get('scaler_sem')
            scaler_tfidf = model_components.get('scaler_tfidf')
            
            if scaler_sem:
                semantic_features = scaler_sem.transform(semantic_features)
            if scaler_tfidf:
                tfidf_features = scaler_tfidf.transform(tfidf_features)
            
            alpha = model_components.get('alpha', 0.5)
            beta = model_components.get('beta', 0.5)
            combined = np.hstack([alpha * semantic_features, beta * tfidf_features])
            
            return clf.predict(combined)[0]
        except Exception as e:
            print(f"‚ö†Ô∏è Model error: {e}")
    
    # Rule-based fallback
    text_lower = text.lower()
    if any(w in text_lower for w in ['‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ', 'hello', 'hi']):
        return "greeting"
    elif any(w in text_lower for w in ['‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì', 'thank']):
        return "thank_you"
    elif any(w in text_lower for w in ['‡∏™‡∏±‡πà‡∏á', '‡∏ã‡∏∑‡πâ‡∏≠', 'order']):
        return "order_product"
    elif any(w in text_lower for w in ['‡∏Ñ‡∏∑‡∏ô', 'refund', '‡πÄ‡∏Ñ‡∏•‡∏°']):
        return "refund_request"
    elif any(w in text_lower for w in ['‡∏ä‡πà‡∏ß‡∏¢', 'help', '‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°']):
        return "help_request"
    elif any(w in text_lower for w in ['‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤', '‡πÄ‡∏™‡∏∑‡πâ‡∏≠', '‡∏Å‡∏≤‡∏á‡πÄ‡∏Å‡∏á', '‡∏£‡∏≠‡∏á‡πÄ‡∏ó‡πâ‡∏≤']):
        return "ask_info"
    elif any(w in text_lower for w in ['‡∏£‡∏µ‡∏ß‡∏¥‡∏ß', 'feedback']):
        return "feedback"
    return "unknown"


def handle_intent_response(intent, user_message, user_id):
    """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"""
    if intent == "greeting":
        return "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏∞ üòä", {}
    
    elif intent == "thank_you":
        return "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞ üôè", {}
    
    elif intent == "ask_info":
        products = get_product_info(user_message, limit=3)
        if products:
            context = "\n".join([
                f"- {p['product_name']}: {p['price']} ‡∏ö‡∏≤‡∏ó, {p['description']}, ‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {p['stock']} ‡∏ä‡∏¥‡πâ‡∏ô"
                for p in products
            ])
            llm_response = call_ollama_llm(user_message, f"‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤:\n{context}")
            if llm_response:
                return llm_response, {"products": products}
            
            product_list = "\n".join([
                f"‚Ä¢ {p['product_name']}\n  üí∞ {p['price']} ‡∏ö‡∏≤‡∏ó\n  üì¶ ‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {p['stock']} ‡∏ä‡∏¥‡πâ‡∏ô"
                for p in products
            ])
            return f"‚ú® ‡∏û‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡∏ô‡πÉ‡∏à:\n\n{product_list}\n\n‡∏™‡∏ô‡πÉ‡∏à‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞?", {"products": products}
        else:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ üòÖ\n\n‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: ‡πÄ‡∏™‡∏∑‡πâ‡∏≠, ‡∏Å‡∏≤‡∏á‡πÄ‡∏Å‡∏á, ‡∏£‡∏≠‡∏á‡πÄ‡∏ó‡πâ‡∏≤, ‡∏Å‡∏£‡∏∞‡πÄ‡∏õ‡πã‡∏≤, ‡∏´‡∏°‡∏ß‡∏Å", {}
    
    elif intent == "order_product":
        orders = get_order_info(user_id=user_id)
        if orders:
            order_list = "\n".join([f"‚Ä¢ {o['order_id']}: {o['status']}" for o in orders[:2]])
            return f"‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:\n{order_list}", {"orders": orders}
        return "‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞? ‡∏ö‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞", {}
    
    elif intent == "refund_request":
        return "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏∑‡∏ô‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 7 ‡∏ß‡∏±‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏à‡πâ‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏Ñ‡πà‡∏∞ üì¶", {}
    
    elif intent == "help_request":
        return "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á:\n‚Ä¢ ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤\n‚Ä¢ ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠\n‚Ä¢ ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏ô‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", {}
    
    elif intent == "feedback":
        return "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Feedback ‡∏Ñ‡∏£‡∏±‡∏ö! ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ô‡∏≥‡πÑ‡∏õ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡πà‡∏∞ üí™", {}
    
    return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡∏Ñ‡∏∞? ü§î", {}


# ===== API ENDPOINTS =====
@app.get("/")
async def root():
    """Health check"""
    return {
        "service": "AI Core",
        "status": "running",
        "model": intent_model if intent_model else "rule-based",
        "version": "1.0.0"
    }


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Main chat endpoint - ‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°
    """
    try:
        print(f"\n{'='*50}")
        print(f"üì® Platform: {request.platform}")
        print(f"üë§ User: {request.user_id}")
        print(f"üí¨ Message: {request.message}")
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Intent
        intent = predict_intent(request.message)
        print(f"üéØ Intent: {intent}")
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        response_text, data = handle_intent_response(
            intent, 
            request.message, 
            request.user_id
        )
        
        print(f"‚úÖ Response: {response_text[:100]}...")
        print(f"{'='*50}\n")
        
        return ChatResponse(
            response=response_text,
            intent=intent,
            confidence=1.0,
            data=data
        )
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "model_loaded": intent_model is not None,
        "timestamp": datetime.now().isoformat()
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)