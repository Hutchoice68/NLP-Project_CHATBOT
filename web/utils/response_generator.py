import random
import json
import subprocess

# ‡πÇ‡∏´‡∏•‡∏î intent responses ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON
with open("intent_responses.json", "r", encoding="utf-8") as f:
    responses = json.load(f)


def generate_response(intent, user_input, context):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ intent ‡πÉ‡∏ô responses.json ‚Üí ‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å preset
    - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å intent ‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Ollama (Llama 3.2)
    """

    # üîπ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô intent_responses.json
    if intent in responses:
        return random.choice(responses[intent])

    # üîπ ‡∏ñ‡πâ‡∏≤ intent ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å ‚Üí ‡πÉ‡∏ä‡πâ Ollama Llama3.2
    prompt = f"""
    ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {user_input}
    ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: {context}

    ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
    ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    """

    try:
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Ollama CLI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• llama3.2
        result = subprocess.run(
            ["ollama", "run", "llama3.2", prompt],
            capture_output=True,
            text=True
        )
        reply = result.stdout.strip()

        if not reply:
            reply = "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡∏£‡∏±‡∏ö ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ üòÖ"

    except Exception as e:
        reply = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}"

    return reply
